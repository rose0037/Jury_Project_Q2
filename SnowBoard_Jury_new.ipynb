{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "################### Old Data Set ###################\n",
    "\n",
    "\n",
    "data = pd.read_csv('jury_data.csv', encoding= 'ISO-8859-1', skiprows=[0,2])\n",
    "data.rename(columns={\"Was defendant Mesa Management negligent?\": \"Mesa_Negligent\", \n",
    "                         \"Was Mesa Management's negligence a substantial factor in causing harm to  Mackenzie Dunn?\":\"Liability\",\n",
    "                         \"What are the total damages that you find that MacKenzie Dunn sufferered?\":\"damages\" ,\n",
    "                         \"What is your sex?\": \"gender\",\n",
    "                         \"Please write your answer to the preceding damages question in words (quality check).\":\"damages_word\",\n",
    "                         \"What percentage of responsibility for Mackenzie Dunn's injuries was each party responsible for? (Answers should add up to 100%) - Mesa Management Co\":\"Mesa_reponsible_percentage\",\n",
    "                         \"Path\":\"Scenario\",\n",
    "                         \"Was MacKenzie Dunn negligent?\":\"Dunn_negligent\",\n",
    "                         \"Unnamed: 63\":\"perc_calc\"\n",
    "                         },inplace=True)\n",
    "data['mm_perc'] = np.where(data['Mesa_reponsible_percentage']>=1, data['perc_calc'], data['Mesa_reponsible_percentage'])\n",
    "req_data = pd.DataFrame(data[[\"Mesa_Negligent\",\"damages\",\"Liability\",\n",
    "                 \"gender\",\n",
    "                 \"damages_word\",\n",
    "                 \"Scenario\",\"Dunn_negligent\",\"perc_calc\",\"Start Date\",\"End Date\",\"mm_perc\"]])\n",
    "\n",
    "\n",
    "req_data['Liability'] = req_data['Liability'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "\n",
    "print(req_data.columns)\n",
    "\n",
    "\n",
    "\n",
    "print(pd.isnull(req_data).any())\n",
    "print(pd.isnull(req_data['Scenario']).any())\n",
    "req_data = req_data[np.isfinite(data['Scenario'])]\n",
    "print(pd.isnull(req_data['Scenario']).any())\n",
    "req_data['damages'].fillna(0,inplace=True)\n",
    "req_data['damages_word'].fillna(0,inplace=True)\n",
    "req_data['mm_perc'].fillna(1,inplace=True)\n",
    "req_data['perc_calc'].fillna(0,inplace=True)\n",
    "#Dropping the last two rows which has null values\n",
    "#data[pd.isnull(data['Path'])]\n",
    "#data['Path']=data.Path.dropna(inplace= True)\n",
    "#data[pd.isnull(data['Path'])]\n",
    "print(pd.isnull(req_data).any())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Changing data types of columns\n",
    "req_data['End Date'] = pd.to_datetime(data['End Date'])\n",
    "req_data['Start Date'] = pd.to_datetime(data['Start Date'])\n",
    "req_data['Scenario']= req_data.Scenario.astype(int)\n",
    "req_data['Liability']= req_data.Liability.astype(int)\n",
    "req_data.dtypes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Getting the id of the column\n",
    "data.columns.get_loc(\"Liability\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cleaning Damages and perc_calc column\n",
    "print(req_data.isnull().any())\n",
    "req_data['damages'] = req_data['damages'].str.replace(',', '')\n",
    "req_data['perc_calc'] = req_data['perc_calc'].str.replace('$', '')\n",
    "req_data['perc_calc'] = req_data['perc_calc'].str.replace(',', '')\n",
    "req_data['perc_calc'] = req_data['perc_calc'].str.replace('-', '')\n",
    "req_data['perc_calc'] = req_data['perc_calc'].str.replace(\"  \", '')\n",
    "req_data['mm_perc'] = req_data['mm_perc'].str.replace(\"$\", '')\n",
    "req_data['mm_perc'] = req_data['mm_perc'].str.replace(\",\", '')\n",
    "req_data['mm_perc'] = req_data['mm_perc'].str.replace(\"  \", '')\n",
    "#req_data.damages=pd.to_numeric(req_data['damages'].str.replace(',', ''))\n",
    "#req_data.perc_calc=pd.to_numeric(req_data.perc_calc)\n",
    "#print(req_data.isnull().any())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "req_data.damages=pd.to_numeric(req_data['damages'])\n",
    "req_data.perc_calc=pd.to_numeric(req_data.perc_calc)\n",
    "req_data['damages'].fillna(0,inplace=True)  \n",
    "req_data['mm_perc'].fillna(1,inplace=True)\n",
    "req_data['perc_calc'].fillna(0,inplace=True)\n",
    "#print(req_data.damages)\n",
    "print(req_data.isnull().any())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(req_data[pd.isnull(req_data['Dunn_negligent'])])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#EDA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "req_data['winrate_percentage']=req_data.Liability\n",
    "req_data['damages_mean']=req_data.damages+req_data.perc_calc\n",
    "req_data['damages_median']=req_data.damages\n",
    "req_data['damages_sd']=req_data.damages\n",
    "\n",
    "winrate_damages_expected=req_data.groupby('Scenario').aggregate(\n",
    "    {'winrate_percentage': np.mean, 'damages_mean': np.mean,'damages_median':np.median,'damages_sd':np.std})\n",
    "\n",
    "\n",
    "winrate_damages_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#req_data['winrate_percentage']=np.mean(req_data.Juror_Response)\n",
    "#print(req_data)\n",
    "req_data['mm_perc'].fillna(1,inplace=True)\n",
    "req_data['damages_mean1']=req_data.damages*pd.to_numeric(req_data.mm_perc)\n",
    "req_data['damages_median1']=req_data.damages\n",
    "req_data['damages_sd1']=req_data.damages\n",
    "#print(req_data.mm_perc)\n",
    "\n",
    "winrate_damages_plaintiffwin=req_data.loc[(req_data['Dunn_negligent']=='No') & (req_data['Liability']==1)].groupby('Scenario').aggregate({'damages_mean1': np.mean,'damages_median1':np.median,'damages_sd1':np.std})\n",
    "\n",
    "\n",
    "winrate_damages_plaintiffwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.factorplot(x='Scenario', y='damages', kind='box',data=req_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.Scenario,data.Liability).plot(kind='bar')\n",
    "plt.title('Purchase Frequency for Job Title')\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Liability')\n",
    "plt.savefig('Juror Response per each Scenario')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = req_data['Scenario']\n",
    "b = req_data['Liability']\n",
    "pd.crosstab(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### New data set ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df =pd.read_csv('Low_Anchor.tsv', sep='\\t+',skiprows=[0,2, 4]+list(range(1,1614,2)) + [1614], names = ['StartDate', 'EndDate',\n",
    "       'ResponseType', \n",
    "       'IP Address', \n",
    "       'Progress', \n",
    "       'Duration',\n",
    "       'Finished',\n",
    "       'RecordedDate',\n",
    "       'ResponseID', \n",
    "       'RecipientLastName','RecipientFirstName','RecipientEmail',\n",
    "       'ExternalDataReference','LocationLatitude', 'LocationLongitude',\n",
    "       'DistributionChannel', 'UserLanguage', 'Participation_in_this_project.',\n",
    "       'Browser Meta Info - Browser',\n",
    "       'Browser Meta Info - Version',\n",
    "       'Browser Meta Info - Operating System',\n",
    "       'Browser Meta Info - Resolution',\n",
    "       'What number did you hear?',\n",
    "       'What word did you see?',\n",
    "       'What is your sex?',\n",
    "       'How old are you?',\n",
    "       'Which of the following best describes your ethnicity?',\n",
    "       'Are you Spanish/Hispanic/Latino',\n",
    "       'What is the highest degree or level of school you have completed?',\n",
    "       'This is an attention check.  Select 200.',\n",
    "       'Which of the following best describes your total household income?',\n",
    "       'Where would you place yourself on this scale?',\n",
    "       'What is your zip code?',\n",
    "       'Timing - First Click','Timing - Last Click','Timing - Page Submit', 'Timing - Click Count',\n",
    "       'Timing - First Click.1', 'Timing - Last Click.1', 'Timing - Page Submit.1',\n",
    "       'Timing - Click Count.1', 'Timing - First Click.2','Timing - Last Click.2',\n",
    "       'Timing - Page Submit.2','Timing - Click Count.2','Timing - First Click.3','Timing - Last Click.3',\n",
    "       'Timing - Page Submit.3','Timing - Click Count.3','Timing - First Click.4', 'Timing - Last Click.4',\n",
    "       'Timing - Page Submit.4','Timing - Click Count.4', 'Timing - First Click.5', 'Timing - Last Click.5',\n",
    "       'Timing - Page Submit.5','Timing - Click Count.5', 'Timing - First Click.6', 'Timing - Last Click.6',\n",
    "       'Timing - Page Submit.6',  'Timing - Click Count.6', 'Timing - First Click.7','Timing - Last Click.7',\n",
    "       'Timing - Page Submit.7',  'Timing - Click Count.7',\n",
    "       'Identify the statement that correctly describes the facts of this case. (This is the attention check)',\n",
    "       'Was_snowboard_sold_McNeil_defective_14', ## using this\n",
    "       \"Is_substantial_factor_McNeil_injuries_14\",\n",
    "       'Non_economic_damages_McNeil_suffered_14',\n",
    "       'Damages_words_14',\n",
    "       'Was_McNeil_negligent',\n",
    "       'McNeil_negligence_substantial_factor_for_injuries',\n",
    "       'Percentage_of_responsibility_X5',\n",
    "       'Percentage_of_responsibility_McNeil',\n",
    "       'Was_snowboard_sold_McNeil_defective_58',\n",
    "       \"Is_substantial_factor_McNeil_injuries_58\",\n",
    "       'Economic_damages_McNeil_suffer_58',\n",
    "       'Economic_Damages_In_Word_58',\n",
    "       'Non_economic_damages_McNeil_suffered_58',\n",
    "       'Non_Economic_Damages_In_Word_58',\n",
    "       'Please explain why you arrived at your decision? (50 character minimum)',\n",
    "       'Q40',#'Did the fact that X5 added core inserts to the later Carve 3000 model, affect your view as to whether the original Carve 3000 was defective?',\n",
    "       'Q41', #'Were you able to ignore the  fact that X5 added core inserts to the later Carve 3000 model when deciding whether the original Carve 3000 was defective?',\n",
    "       'Path'])\n",
    "        \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## replacing hexadecimal value of damages'/x00' to ''\n",
    "for i in range(len(df)):\n",
    "    df['Was_snowboard_sold_McNeil_defective_14'].values[i] = df['Was_snowboard_sold_McNeil_defective_14'].values[i].replace('\\x00','')\n",
    "    df['Is_substantial_factor_McNeil_injuries_14'].values[i] = df['Is_substantial_factor_McNeil_injuries_14'].values[i].replace('\\x00','')\n",
    "    df['Non_economic_damages_McNeil_suffered_14'].values[i] = df['Non_economic_damages_McNeil_suffered_14'].values[i].replace('\\x00','')\n",
    "    df['Damages_words_14'].values[i] = df['Damages_words_14'].values[i].replace('\\x00','')\n",
    "    df['Was_McNeil_negligent'].values[i] = df['Was_McNeil_negligent'].values[i].replace('\\x00','') ;\n",
    "    df['McNeil_negligence_substantial_factor_for_injuries'].values[i] = df['McNeil_negligence_substantial_factor_for_injuries'].values[i].replace('\\x00','') ;\n",
    "    df['Percentage_of_responsibility_X5'].values[i] = df['Percentage_of_responsibility_X5'].values[i].replace('\\x00','') ;\n",
    "    df['Percentage_of_responsibility_McNeil'].values[i] = df['Percentage_of_responsibility_McNeil'].values[i].replace('\\x00','') ;\n",
    "    df['Was_snowboard_sold_McNeil_defective_58'].values[i] = df['Was_snowboard_sold_McNeil_defective_58'].values[i].replace('\\x00','') ;\n",
    "    df['Is_substantial_factor_McNeil_injuries_58'].values[i] = df['Is_substantial_factor_McNeil_injuries_58'].values[i].replace('\\x00','') ;\n",
    "    df['Economic_damages_McNeil_suffer_58'].values[i] = df['Economic_damages_McNeil_suffer_58'].values[i].replace('\\x00','') ;\n",
    "    df['Economic_Damages_In_Word_58'].values[i] = df['Economic_Damages_In_Word_58'].values[i].replace('\\x00','') ;\n",
    "    df['Non_economic_damages_McNeil_suffered_58'].values[i] = df['Non_economic_damages_McNeil_suffered_58'].values[i].replace('\\x00','') ;\n",
    "    df['Non_Economic_Damages_In_Word_58'].values[i] = df['Non_Economic_Damages_In_Word_58'].values[i].replace('\\x00','') ;\n",
    "    df['Path'].values[i] = df['Path'].values[i].replace('\\x00','') ;  \n",
    "    df['Q40'].values[i] = df['Q40'].values[i].replace('\\x00','') ; \n",
    "    #df['Was the Carve 3000 snowboard X5 sold Connor McNeil defective?'].values[i] =  df['Was the Carve 3000 snowboard X5 sold Connor McNeil defective?'].values[i].replace('\\x00','') ;  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing Data type\n",
    "df.StartDate = pd.to_datetime(df.StartDate)\n",
    "df.EndDate   = pd.to_datetime(df.EndDate) \n",
    "#df.Was_snowboard_sold_McNeil_defective_14   = pd.to_numeric(df.Was_snowboard_sold_McNeil_defective_14)\n",
    "df.Is_substantial_factor_McNeil_injuries_14 = pd.to_numeric(df.Is_substantial_factor_McNeil_injuries_14)\n",
    "df.Non_economic_damages_McNeil_suffered_14  = pd.to_numeric(df.Non_economic_damages_McNeil_suffered_14)\n",
    "df.Was_McNeil_negligent                     = pd.to_numeric(df.Was_McNeil_negligent)\n",
    "df.McNeil_negligence_substantial_factor_for_injuries= pd.to_numeric(df.McNeil_negligence_substantial_factor_for_injuries)\n",
    "df.Percentage_of_responsibility_X5          = pd.to_numeric(df.Percentage_of_responsibility_X5)\n",
    "df.Percentage_of_responsibility_McNeil      = pd.to_numeric(df.Percentage_of_responsibility_McNeil)\n",
    "#df.Was_snowboard_sold_McNeil_defective_58   = pd.to_numeric(df.Was_snowboard_sold_McNeil_defective_58)\n",
    "df.Is_substantial_factor_McNeil_injuries_58 = pd.to_numeric(df.Is_substantial_factor_McNeil_injuries_58)\n",
    "df.Economic_damages_McNeil_suffer_58        = pd.to_numeric(df.Economic_damages_McNeil_suffer_58)\n",
    "df.Non_economic_damages_McNeil_suffered_58  = pd.to_numeric(df.Non_economic_damages_McNeil_suffered_58)\n",
    "df.Q40 =pd.to_numeric(df.Q40) \n",
    "# Handling for Path\n",
    "df.Path = pd.to_numeric(df.Path) \n",
    "df['Path'].fillna(0,inplace = True)\n",
    "df.Path =  df.Path.astype(int)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the required columns and storing it in \"newdf\" data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newdf =pd.DataFrame(df[['StartDate', 'EndDate',\n",
    "       'Was_snowboard_sold_McNeil_defective_14', \n",
    "       \"Is_substantial_factor_McNeil_injuries_14\",\n",
    "       'Non_economic_damages_McNeil_suffered_14',                                                                                         \n",
    "       'Was_McNeil_negligent',\n",
    "       'McNeil_negligence_substantial_factor_for_injuries',                                                                                         \n",
    "       'Percentage_of_responsibility_X5',\n",
    "       'Percentage_of_responsibility_McNeil'                                                                                      ,\n",
    "       'Was_snowboard_sold_McNeil_defective_58',\n",
    "       \"Is_substantial_factor_McNeil_injuries_58\",\n",
    "       'Economic_damages_McNeil_suffer_58',\n",
    "       'Non_economic_damages_McNeil_suffered_58',\n",
    "       'Q40',\n",
    "       'Path']])\n",
    "        \n",
    "##newdf.head(5)\n",
    "newdf.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answering to the question.\n",
    "\n",
    "\n",
    "<font color = red>\n",
    "With respect to the first question, I realize that answers from participants in versions 1 and 5 are meaningless.  They did not see evidence of added core inserts. As far as the analysis, I think we want to see if this answer predicted how people responded to the liability questions. For example, did people that said \"Yes this evidence strongly suggested the Carve 3000 was defective” find liability more often than people that answered “No”. \n",
    "</font>\n",
    "\n",
    "Here Q40 is \"Did the fact that X5 added core inserts to the later Carve 3000 model, affect your view as to whether the original Carve 3000 was defective?\"\n",
    "\n",
    "The Values are: \n",
    "- 1 = Yes, it strongly suggested that the original Carve 3000 was defective.\n",
    "- 2 = Yes, it somewhat suggested that the original Carve 3000 was defective.\n",
    "- 3 = No, it did not suggest that the original Carve 3000 was defective.\n",
    "\n",
    "The value for \"Was_snowboard_sold_McNeil_defective_14\" i:e the liability from Path 1 to 4 , are 4 and 6\n",
    "- 4 = Yes\n",
    "- 6 = No\n",
    "\n",
    "So first lets check, how many juror who <font color = blue> strongly suggested that the original Carve 3000 was defective </font> found the snowboard sold to McNeil was defective i:e the liability as <font color = green> Yes (4) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So There 50 observations with Liability as 'Yes'. Now lets see how many juror <font color = blue> who strongly suggested that the original Carve 3000 was defective </font> responded liability as <font color = red> No (6) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So There no observations with Liability as 'No'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check number of juror who said <font color = blue> No, it did not suggest that the original Carve 3000 was defective (3) </font> found the snowboard sold to McNeil was defective i:e the liability as <font color = green >Yes (4)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = newdf['Q40']\n",
    "b = newdf['Was_snowboard_sold_McNeil_defective_14']\n",
    "pd.crosstab(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how many missing data points we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok, now we know that we do have some missing values. Let's see how many we have in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "missing_values_count = newdf.isnull().sum()\n",
    "\n",
    "print(missing_values_count)\n",
    "\n",
    "total_cells = np.product(newdf.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "(total_missing/total_cells) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newdf.isnull().sum()\n",
    "newdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we are just working on from path 1 to 8, Lets remove path with value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf[newdf.Path <=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf[newdf.Path <=0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red' size = \"5\"> As we can see there are 13 observation with path value equal to 0. We are removing these observation </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed path with 0 values \n",
    "newdf = newdf[newdf.Path > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing the Null Values with empty string(Easy to convert to other datatypes Later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(pd.isnull(newdf).any())\n",
    "newdf = newdf[np.isfinite(newdf['Path'])]\n",
    "newdf['Is_substantial_factor_McNeil_injuries_14'].fillna(\"\",inplace=True)\n",
    "newdf['Non_economic_damages_McNeil_suffered_14'].fillna(\"\",inplace=True)\n",
    "newdf['Was_McNeil_negligent'].fillna(\"\",inplace=True)\n",
    "newdf['McNeil_negligence_substantial_factor_for_injuries'].fillna(\"\",inplace=True)\n",
    "newdf['Percentage_of_responsibility_X5'].fillna(\"\",inplace=True)\n",
    "newdf['Percentage_of_responsibility_McNeil'].fillna(\"\",inplace=True)\n",
    "newdf['Was_snowboard_sold_McNeil_defective_58'].fillna(\"\",inplace=True)\n",
    "newdf['Is_substantial_factor_McNeil_injuries_58'].fillna(\"\",inplace=True)\n",
    "newdf['Economic_damages_McNeil_suffer_58'].fillna(\"\",inplace=True)\n",
    "newdf['Non_economic_damages_McNeil_suffered_58'].fillna(\"\",inplace=True)\n",
    "# Printing the first 5 lines.\n",
    "newdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing datatype of damages and filling NULL values with 0s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the requirement we have to calculate the Total Damages for each path.\n",
    "There are 8 different Paths. \n",
    "\n",
    "- Path 1 2 3 4 : Scenarios with no Low Anchor. \n",
    "- Path 5 6 7 8 : Scenarios with Low Anchor. \n",
    "\n",
    "As we are not taking consideration of Low Anchor,we renamed Path 5,6,7,8 as 1,2,3,4 respectively. \n",
    "\n",
    "Later we converted the data type of Path as \"Int\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['Path'].replace([5, 6 ,7,8], [1,2,3,4], inplace = True)\n",
    "newdf['Path']= newdf.Path.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>We need to change the data type of damages. There are 3 different columns that have the damages \n",
    "information. From previous data type check, we found that there are so many missing values for damages.\n",
    "So we replaced them with 0.\n",
    "</font>\n",
    "\n",
    "For simplicity to plot Path vs damages we combined all damages into one column and named it \n",
    "as \"Total_Damages\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.Economic_damages_McNeil_suffer_58        = pd.to_numeric(newdf.Economic_damages_McNeil_suffer_58)\n",
    "newdf.Non_economic_damages_McNeil_suffered_58  = pd.to_numeric(newdf.Non_economic_damages_McNeil_suffered_58)\n",
    "newdf.Non_economic_damages_McNeil_suffered_14  = pd.to_numeric(newdf.Non_economic_damages_McNeil_suffered_14) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Path vs Economic Damages.\n",
    "\n",
    "#### We have economic damages only from Path 5 to 8, so ploting the graph for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "newdf58 = df[df.Path>4]\n",
    "plt = sns.factorplot(x='Path', y='Economic_damages_McNeil_suffer_58', kind='box',data=newdf58, size=5)\n",
    "_ = plt.set(xlabel='Path', ylabel='Economic Damages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Path(5 to 8) vs Non Economic Damages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt1 = sns.factorplot(x='Path', y='Non_economic_damages_McNeil_suffered_58', kind='box',data=newdf58, size=6)\n",
    "_ = plt1.set(xlabel='Path', ylabel='Non Economic Damages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Path(1 to 4) vs Non Economic Damages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt2 = sns.factorplot(x='Path', y='Non_economic_damages_McNeil_suffered_14', kind='box',data=newdf, size=5)\n",
    "_ = plt2.set(xlabel='Path', ylabel='Non Economic Damages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column for non economic damages(for path 1 to 4 and path 5 to 8--do boxplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Filling the NaN values with 0, first lets check if any juror has put 0 intentionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.query('Non_economic_damages_McNeil_suffered_14 == 0 | Non_economic_damages_McNeil_suffered_58 == 0 |Economic_damages_McNeil_suffer_58 ==0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> We found that one row has 0 value for Non_economic damages McNeil suffered. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.Economic_damages_McNeil_suffer_58.fillna(0, inplace = True)\n",
    "newdf.Non_economic_damages_McNeil_suffered_58.fillna(0, inplace = True)\n",
    "newdf.Non_economic_damages_McNeil_suffered_14.fillna(0, inplace = True)\n",
    "\n",
    "newdf['Total_Damages'] =  newdf['Economic_damages_McNeil_suffer_58']+newdf['Non_economic_damages_McNeil_suffered_58'] + newdf['Non_economic_damages_McNeil_suffered_14']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box Plot for Total Damages vs Path.\n",
    "\n",
    "We used Violin Plot because it allows a deeper understanding of the density. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(newdf.Total_Damages==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import matplotlib.axes as p\n",
    "\n",
    "\n",
    "newdf[\"Path\"] = newdf.Path.astype('category')\n",
    "#_=sns.factorplot(x='Path', y='Total_Damages', kind='violin',data=newdf, size=5)#.set(ylim=0)\n",
    "#sns.factorplot(x='Path', y='Total_Damages',kind='violin',data=newdf, size=5)#.fit_kde\n",
    "\n",
    "sns.violinplot(x=\"Path\", y=\"Total_Damages\", data=newdf, inner = 'box')\n",
    "\n",
    "# Note: This happens because to calculate the lowest whisker you use : \n",
    "#The values for Q1 – 1.5×IQR are the \"fences\" that mark off the \"reasonable\" \n",
    "# values from the outlier values.  gaussian_kde works for both uni-variate and multi-variate data. \n",
    "# It includes automatic bandwidth determination.\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the box plot, we can see there are some outlier for Path 1 and 4. Lets find what are the outliers are.\n",
    "For Path column in newdf, we can get 0.99 quantile and then we printed the rows having outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_df = newdf[newdf.Total_Damages >0]\n",
    "sns.factorplot(x='Path', y='Total_Damages', kind='box',data=damage_df, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(damage_df.Total_Damages <0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**based on above boxplot path 4 has an outlier value around 1000000. Hence we will remove this value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot(x='Path', y='Total_Damages', kind='violin',data=damage_df, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = newdf[\"Total_Damages\"].quantile(0.99)\n",
    "print(q)\n",
    "newdf.query(\"Total_Damages >= 500000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Lets remove the outlier and plot the box plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf1 = newdf[(newdf.Total_Damages < q) & (newdf.Total_Damages >0) ]\n",
    "sns.factorplot(x='Path', y='Total_Damages', kind='box',data=newdf1, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = newdf1[\"Total_Damages\"].quantile(0.99)\n",
    "print(q)\n",
    "newdf1.query(\"(Total_Damages >= 500000 & Path ==1)|(Total_Damages <500000)\")\n",
    "newdf2 = newdf1.drop(newdf1[(newdf1.Total_Damages >= 500000) & (newdf1.Path == 1)].index)\n",
    "\n",
    "#sns.factorplot(x='Path', y='Total_Damages', kind='box',data=newdf2, size=5)\n",
    "#newdf1[(newdf1.Total_Damages >= 500000) & (newdf1.Path == 1)]\n",
    "print(newdf1.shape)\n",
    "print(newdf2.shape)\n",
    "\n",
    "q1 = newdf2[\"Total_Damages\"].quantile(0.99)\n",
    "print(q1)\n",
    "newdf2.query(\"Total_Damages >= 500000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['Liability'] = newdf['Was_snowboard_sold_McNeil_defective_14'] + newdf['Was_snowboard_sold_McNeil_defective_58']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For graph we are changing the value 4 and 6 to Yes and No. Liability with blank is replace with \"No Reponse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['Liability'].replace(['4', '6' , ''], ['Yes','No', 'No Reponse'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning data(checking if any column has null values)\n",
    "newdf.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph showing the responses of jurors for each path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Juror Responce vs Path\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "pd.crosstab(newdf.Path,newdf.Liability).plot(kind='bar', fontsize = 15, figsize=(10,6))\n",
    "plt.title('Liability Vs Path Graph')\n",
    "plt.xlabel('Path')\n",
    "plt.ylabel('Liability')\n",
    "plt.savefig('Juror Response vs Path')\n",
    "\n",
    "a = newdf['Path']\n",
    "b = newdf['Liability']\n",
    "pd.crosstab(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding winrate percentage for each path \n",
    "ratedf=pd.DataFrame(newdf[['Liability','Path','Was_McNeil_negligent']])\n",
    "ratedf['winrate_percentage']=ratedf.Liability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Winrate, Expected Damages, mean , median and SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ratedf['winrate_percentage']=newdf.Liability\n",
    "ratedf['damages_mean']=pd.to_numeric(newdf.Non_economic_damages_McNeil_suffered_14) + pd.to_numeric(newdf.Non_economic_damages_McNeil_suffered_58) \n",
    "ratedf['damages_median']=pd.to_numeric(newdf.Non_economic_damages_McNeil_suffered_14 )+pd.to_numeric( newdf.Non_economic_damages_McNeil_suffered_58) \n",
    "ratedf['damages_sd']=pd.to_numeric(newdf.Non_economic_damages_McNeil_suffered_14) + pd.to_numeric(newdf.Non_economic_damages_McNeil_suffered_58)\n",
    "ratedf['winrate_percentage'] = ratedf['winrate_percentage'].map({\"Yes\":1, \"No\":0})\n",
    "\n",
    "winrate_damages_expected=ratedf.groupby('Path').aggregate(\n",
    "    {'winrate_percentage': np.mean\n",
    "     ,'damages_mean': np.mean\n",
    "     ,'damages_median':np.median\n",
    "     ,'damages_sd':np.std\n",
    "    })\n",
    "\n",
    "winrate_damages_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Damages, mean , median and SD when plaintiff wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "winrate_damages_plaintiffwin = ratedf.loc[(ratedf['Was_McNeil_negligent']== 1) & (ratedf['Liability']=='Yes')].groupby('Path').aggregate(\n",
    "    {'damages_mean': np.mean\n",
    "     ,'damages_median':np.median\n",
    "     ,'damages_sd':np.std\n",
    "    })\n",
    "winrate_damages_plaintiffwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.columns\n",
    "newdf1=pd.DataFrame(newdf[[\"StartDate\",\"EndDate\",\"Liability\",'Total_Damages','Path','Was_McNeil_negligent']])\n",
    "newdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf1.rename(columns={\"StartDate\": \"Start Date\", \n",
    "                         \"EndDate\":\"End Date\",\n",
    "                         \"Total_Damages\":\"damages\",\n",
    "                       \"Was_McNeil_negligent\":\"Plaintiff_negligent\"\n",
    "                         },inplace=True)\n",
    "\n",
    "newdf1['Plaintiff_negligent'] = newdf1['Plaintiff_negligent'].map({1:\"Yes\", 2:\"No\"})\n",
    "newdf1['Liability'] = newdf1['Liability'].map({\"Yes\":1, \"No\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_data.columns\n",
    "req_data1=pd.DataFrame(req_data[[\"Start Date\",\"End Date\",\"Liability\",'damages','Scenario','perc_calc','mm_perc','Dunn_negligent']])\n",
    "req_data1.rename(columns={\n",
    "                       \"Scenario\":\"Path\",\"Dunn_negligent\":\"Plaintiff_negligent\"\n",
    "                         },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=[newdf1,req_data1]\n",
    "merge_data = pd.concat(frames, keys=['x', 'y'])\n",
    "\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case Expected Value Damages for the merge data\n",
    "Showing the total expected damages mean,median and sd with winrate percentage (entire version)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data\n",
    "merge_data['winrate_percentage']=merge_data.Liability\n",
    "merge_data['damages_mean']=merge_data.damages+merge_data.perc_calc\n",
    "merge_data['damages_median']=merge_data.damages\n",
    "merge_data['damages_sd']=merge_data.damages\n",
    "\n",
    "\n",
    "\n",
    "winrate_damages_expected=merge_data.groupby('Path').aggregate(\n",
    "    {'winrate_percentage': np.mean, 'damages_mean': np.mean,'damages_median':np.median,'damages_sd':np.std})\n",
    "\n",
    "\n",
    "winrate_damages_expected.winrate_percentage*=100\n",
    "winrate_damages_expected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To retrive data based on the keys:\n",
    "merge_data.loc['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Damages, mean , median and SD when plaintiff wins for the merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#req_data['winrate_percentage']=np.mean(req_data.Juror_Response)\n",
    "#print(req_data)\n",
    "merge_data['mm_perc'].fillna(1,inplace=True)\n",
    "merge_data['damages_mean1']=merge_data.damages*pd.to_numeric(merge_data.mm_perc)\n",
    "merge_data['damages_median1']=merge_data.damages\n",
    "merge_data['damages_sd1']=merge_data.damages\n",
    "#print(req_data.mm_perc)\n",
    "\n",
    "winrate_damages_plaintiffwin=merge_data.loc[(merge_data['Plaintiff_negligent']=='No') & (merge_data['Liability']==1)].groupby('Path').aggregate({'damages_mean1': np.mean,'damages_median1':np.median,'damages_sd1':np.std})\n",
    "\n",
    "\n",
    "winrate_damages_plaintiffwin\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
